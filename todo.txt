================================================================================
FACTZCHECK ‚Äî NLP SERVICE COMPLETE GUIDE
================================================================================

FILES IMPLEMENTED:
  - nlp-service/main.py      ‚Äî Full FastAPI app with 3 endpoints (/health, /extract, /metrics)
  - nlp-service/tests/test_extractor.py ‚Äî 22 test cases covering year, value, metric, and full pipeline extraction

TO RUN THE NLP SERVICE:
  cd nlp-service
  pip install fastapi uvicorn pytest
  python main.py
  Then open http://localhost:5001/docs for interactive Swagger UI.

TO RUN TESTS:
  cd nlp-service
  python -m pytest tests/ -v

================================================================================
PART 1: APIs & DATASETS FOR VERIFYING CLAIMS WITH REAL DATA
================================================================================

Your official_data_cache table needs real official numbers. Here's where to get them:

FREE GOVERNMENT & INSTITUTIONAL APIs:

  1. World Bank Open Data
     - Data: GDP, inflation, unemployment, poverty, literacy ‚Äî ALL countries
     - URL: https://api.worldbank.org/v2/country/IND/indicator/NY.GDP.MKTP.KD.ZG?format=json&date=2020:2025
     - Format: REST JSON
     - NO API KEY NEEDED

  2. RBI (Reserve Bank of India)
     - Data: Forex reserves, CPI inflation, monetary data
     - URL: https://data.rbi.org.in/ (DBIE portal)
     - Format: CSV/Excel download

  3. data.gov.in
     - Data: India-specific: literacy, population, fiscal data
     - URL: https://data.gov.in/search (API key required ‚Äî free)
     - Format: REST JSON

  4. IMF Data API
     - Data: GDP, fiscal deficit, trade deficit, current account
     - URL: https://www.imf.org/external/datamapper/api/v1/
     - Format: REST JSON

  5. FRED (Federal Reserve)
     - Data: Global economic indicators (backup source)
     - URL: https://api.stlouisfed.org/fred/series/observations?series_id=...&api_key=YOUR_KEY
     - Format: REST JSON

WORLD BANK INDICATOR CODES FOR YOUR 10 METRICS:

  GDP growth rate           ‚Üí NY.GDP.MKTP.KD.ZG
  Inflation rate            ‚Üí FP.CPI.TOTL.ZG
  Unemployment rate         ‚Üí SL.UEM.TOTL.ZS
  Fiscal deficit            ‚Üí GC.BAL.CASH.GD.ZS
  Literacy rate             ‚Üí SE.ADT.LITR.ZS
  Population                ‚Üí SP.POP.TOTL
  Per capita income         ‚Üí NY.GDP.PCAP.CD
  Poverty rate              ‚Üí SI.POV.NAHC
  Foreign exchange reserves ‚Üí FI.RES.TOTL.CD
  Current account deficit   ‚Üí BN.CAB.XOKA.GD.ZS

Example API call:
  GET https://api.worldbank.org/v2/country/IND/indicator/NY.GDP.MKTP.KD.ZG?format=json&date=2020:2025

DATA FLOW:
  On Startup / Scheduled Job ‚Üí Fetch from World Bank/RBI/IMF APIs
                             ‚Üí INSERT/UPDATE into official_data_cache
                             ‚Üí Used for comparison when user submits a claim

================================================================================
PART 2: FULL INTEGRATION ARCHITECTURE
================================================================================

  User Browser (Frontend React :3000)
       ‚îÇ HTTP POST /api/claims
       ‚ñº
  Backend (Node.js/Express :5000)
       ‚îÇ 1. Auth middleware: verify JWT
       ‚îÇ 2. Save claim to DB (status: 'pending')
       ‚îÇ 3. POST http://localhost:5001/extract ‚Üí NLP Service
       ‚îÇ 4. Receive {metric, value, year}
       ‚îÇ 5. Query official_data_cache for (metric, year)
       ‚îÇ 6. Calculate difference, percentage_error, verdict
       ‚îÇ 7. INSERT into verification_log
       ‚îÇ 8. UPDATE claims SET credibility_score, status='verified'
       ‚îÇ 9. Return result to frontend
       ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ                     ‚îÇ
  ‚ñº                     ‚ñº
  NLP Service       MySQL Database
  (Python :5001)    (localhost:3306)
  POST /extract     users, claims,
                    official_data_cache,
                    verification_log

INTEGRATION STEPS (CHECKLIST):

  [ ] I-1  Set up MySQL DB, run schema.sql
  [ ] I-2  Build config/db.js ‚Äî MySQL connection pool
  [ ] I-3  Build server.js ‚Äî Express app setup
  [ ] I-4  Build middleware/auth.js ‚Äî JWT verification
  [ ] I-5  Build controllers/authController.js ‚Äî register/login
  [ ] I-6  Build routes/authRoutes.js
  [ ] I-7  Build controllers/claimController.js ‚Äî calls NLP service
  [ ] I-8  Build routes/claimRoutes.js
  [ ] I-9  Build data seeder ‚Äî fetch World Bank data ‚Üí official_data_cache
  [ ] I-10 Build frontend (React) ‚Äî login, submit claim, view results
  [ ] I-11 End-to-end testing

KEY INTEGRATION CODE (claimController.js):

  // Backend calls Python NLP service
  const nlpResponse = await fetch('http://localhost:5001/extract', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text: req.body.claim_text })
  });
  const { metric, value, year, confidence } = await nlpResponse.json();

  // Look up official data
  const [rows] = await db.query(
      'SELECT value FROM official_data_cache WHERE metric_name = ? AND year = ?',
      [metric, year]
  );
  const officialValue = rows[0].value;
  const difference = Math.abs(value - officialValue);
  const percentageError = (difference / officialValue) * 100;
  const verdict = percentageError < 5 ? 'accurate'
                : percentageError < 20 ? 'misleading'
                : 'false';

================================================================================
PART 3: REGEX-CACHE vs SELF-TRAINED MODEL ‚Äî TRADEOFFS
================================================================================

COMPARISON TABLE:

  Dimension       | Regex (Cache)         | ML Model              | Hybrid
  ----------------+-----------------------+-----------------------+------------------
  Speed           | ~1ms                  | ~50-500ms             | Fast for known
  Accuracy        | 70-80%                | 90-95%                | 85-95%
  Flexibility     | Only written patterns | Handles novel phrases | Best of both
  Data needed     | None                  | 5,000-50,000 labeled  | Some for ML part
  Cost            | Free (CPU)            | GPU for training      | Mixed
  Maintenance     | Manual pattern adds   | Retrain periodically  | Both
  Transparency    | 100% explainable      | Black box             | Mixed
  Cold start      | Works immediately     | Needs weeks of prep   | Regex works day 1

RECOMMENDED APPROACH: HYBRID, PHASED

  Phase 1 (NOW ‚Äî MVP):
    ‚úÖ Regex extraction (what we just built)
    ‚úÖ Direct comparison against official_data_cache
    ‚úÖ Ship it, get users, collect real claims

  Phase 2 (After ~1000 real claims collected):
    üìä Use collected claims as training data
    üìä Label them: "did regex get the right metric/value/year?"
    üìä Fine-tune a small model (DistilBERT or SpaCy NER)
    üìä Deploy as fallback when regex confidence < 0.6

  Phase 3 (Scale):
    üöÄ ML model becomes primary extractor
    üöÄ Regex becomes the fast cache
    üöÄ If regex confidence >= 0.9 ‚Üí skip ML, return immediately
    üöÄ If regex confidence < 0.9 ‚Üí call ML model

DATASETS FOR FUTURE ML TRAINING:

  LIAR dataset     ‚Äî 12,800 labeled political claims (true/false)
                     GitHub: github.com/talhbutt/LIAR-PLUS
  FEVER            ‚Äî 185,000 claims against Wikipedia evidence
                     URL: fever.ai
  ClaimBuster      ‚Äî Claims from US presidential debates
                     URL: idir.uta.edu/claimbuster/
  MultiFC          ‚Äî 34,000 claims from 26 fact-checking sites
  Your own data    ‚Äî Real claims from FACTZcheck users (BEST source)

MODEL RECOMMENDATIONS FOR PHASE 2:

  SpaCy custom NER    ‚Äî Python-native, fast, lightweight (RECOMMENDED)
                        Label ~500-1000 examples, train in <1 hour on CPU
  DistilBERT fine-tune ‚Äî Very good accuracy, needs GPU, medium complexity
  BERT + CRF layer    ‚Äî Best for extraction, high complexity
  GPT/Gemini API      ‚Äî No training needed, but costs $$$ per request

================================================================================
NEXT STEP: Build the backend (I-1 through I-9)
================================================================================
